\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
%\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

% ---------------------
% Pacotes OBRIGATÓRIOS
% ---------------------
%\usepackage{lmodern}				% Usa a fonte Latin Modern			
\usepackage[T1]{fontenc}			% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)
%\usepackage{lastpage}			% Usado pela Ficha catalográfica
%\usepackage{indentfirst}			% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}			     	% Controle das cores
\usepackage{graphicx}	         % Inclusão de gráficos
\usepackage{epsfig,subfig}		% Inclusão de figuras
\usepackage{microtype} 			% Melhorias de justificação
% ---------------------

% ---------------------
% Pacotes ADICIONAIS
% ---------------------
\usepackage{lipsum}						% Geração de dummy text
\usepackage{amsmath,amssymb,mathrsfs, amsthm}	% Comandos matemáticos avançados 
\usepackage{setspace}  					% Para permitir espaçamento simples, 1 1/2 e duplo
\usepackage{verbatim}					% Para poder usar o ambiente "comment"
\usepackage{tabularx} 					% Para poder ter tabelas com colunas de largura auto-ajustável
\usepackage{afterpage} 					% Para executar um comando depois do fim da página corrente
\usepackage{url} 						% Para formatar URLs (endereços da Web)
\usepackage{todonotes}  		% Lista de afazeres To-dos
\usepackage{enumitem}					% Fazer enumerações por letras ou números nos itens
\usepackage{float}
\usepackage{longtable}					% Tabelas que se estendem por mais de uma página

% Configura o ambiente de teoremas, definições etc.

\theoremstyle{definition} 
\newtheorem{teor}{Teorema}%[section]
\newtheorem{defi}[teor]{Definição}
\newtheorem{lema}[teor]{Lema}
\newtheorem{supo}[teor]{Suposição}
\newtheorem{exemplo}[teor]{Exemplo}
\newtheorem{prop}[teor]{Proposição}


\begin{document}

\begin{frontmatter}

\title{Medidas condicionais de risco através da teoria do valor extremo\tnoteref{mytitlenote}}
\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
%\author{Rafael Felipe Bressan\fnref{myfootnote}\corref{correspondente}}
%\address{Avenida Madre Benvenuta, 2007 - Santa Mônica Florianópolis - SC 88035-901}
%\fntext[myfootnote]{Depto. de Economia/Esag/UDESC}

%% or include affiliations in footnotes:
\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
\ead[url]{www.elsevier.com}

\author[mymainaddress]{Rafael Felipe Bressan\corref{mycorrespondingauthor}\fnref{myfootnote}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{rafael.bressan@edu.udesc.br}
\fntext[myfootnote]{Depto. de Economia/Esag/UDESC}

\address[mymainaddress]{Avenida Madre Benvenuta, 2007 - Santa Mônica Florianópolis - SC 88035-901}
\address[mysecondaryaddress]{360 Park Avenue South, New York}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.
\end{abstract}

\begin{keyword}
\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introdução}

\paragraph{A medição do risco de mercado} ao qual os portfólios dos investidores está sujeito é objeto de devoção de esforços tanto por parte das instituições e investidores em geral como por parte dos reguladores. Instituições financeiras em todo o mundo, de acordo com suas regulações locais e com os princípios de Basileia (\emph{Basel Comittee on Banking Supervision} - BCBS do Banco de Compensações Internacionais - BIS\footnote{http://www.bis.org/bcbs/index.htm?m=3\%7C14}) para aquelas que o seguem (o Brasil é um desses países)  são obrigadas a reservar uma parcela de seu capital como provisionamento contra flutuações adversas do mercado em seus portfólios de investimento.

Uma importante característica das séries de retornos financeiros é sua alta volatilidade, não constante e tampouco seguindo a distribuição Normal. Assim, eventos extremos, e neste caso estamos interessados em perdas de grande magnitude, acontecem com uma frequência alta demais para serem descartadas como apenas \emph{outliers}, e portanto passaram a atrair a atenção dos participantes do mercado, entre eles os investidores e também os reguladores. Estas observações induziram uma gama enorme de estudos, práticos e teóricos, voltados a explicar o comportamento dos retornos de séries financeiras e modelar de forma adequada as caudas da distribuição destes retornos. Não somente estes estudos são de grande relevância para o gerenciamento de risco nas instituições financeiras, como também são obrigatórios segundo o acordo de Basileia, uma vez que este requer o cálculo do Valor em Risco - VaR, para então a instituição poder projetar o seu nível requerido de capital. 

De acordo com os princípios de Basileia III, \cite{BankingSupervision2011, BankingSupervision2013, BankingSupervision2014}, as instituições financeiras supervisionadas pelos Bancos Centrais devem manter \emph{buffers} de capital contra riscos de mercado, crédito, liquidez, entre outros. Dentro dos riscos de mercado, as duas formas mais usuais de fazer a quantificação destes são os métodos de Valor em Risco - VaR e o \emph{Expected Shortfall} - ES. Este último relacionado ao primeiro, sendo definido como o valor esperado das perdas que excedem o valor VaR calculado para um determinado nível de confiança.
\todo{Definição de VaR. Nível ou intervalo de confiança? verificar em riskmetrics}
\todo{Percentuais de VaR por Basileia}
\todo{Existem penalidades regulatórias para as IF em que seu modelo VaR permite um número maior de perdas do que seria estimado pelo modelo. Verificar onde nos princípios de Basileia}

VaR é um quantil alto $\alpha$ da distribuição de perdas de um ativo ou portfólio em um determinado período de tempo, ao passo que ES é o valor esperado das perdas que excedem VaR, para um mesmo nível de confiança $\alpha$ e período.

O método VaR para cálculo de risco de mercado ao qual um portfólio está sujeito foi primeiramente introduzido através de \cite{RiskMetrics1995}, uma metodologia adotada pelo banco J. P. Morgan. Vem desde então sendo amplamente adotado pela indústria financeira e largamente estudado pela academia. Inúmeras variantes do modelo foram propostas e continuam sendo utilizadas com o passar dos anos. Para o cálculo do VaR é necessária uma suposição acerca da distribuição dos retornos, e por conseguinte do comportamento da cauda desta.

As variações na metodologia original de estimação do VaR surgem principalmente em função de críticas a abordagem proposta, a qual inclui a suposição de retornos independentes e igualmente distribuídos, covariâncias constantes entre os ativos de um portfólio e a distribuição normal dos retornos.

Por meio de dois artigos \cite{Artzner1997} e \cite{Artzner1999}, foi introduzido na literatura o conceito de medida coerente de risco. Para uma medida ser considerada coerente, primeiramente foram introduzidas quatro propriedades cunhadas através de axiomas, as quais estas medidas deveriam possuir, sendo elas: 

\begin{itemize}
	\item invariância a translação;
	\item sub-aditividade;
	\item homogeneidade positiva, e;
	\item monotonicidade.
\end{itemize}

VaR especificamente não possui a propriedade da sub-aditividade para alguns casos, sendo esta uma das grandes críticas ao VaR. Para contornar este fato, \cite{Acerbi2002} propõe o \emph{Expected Shortfall} e comprovam que este é uma medida coerente de risco. Além de ser coerente, o ES possui uma segunda vantagem com relação ao VaR, considerando que o ES nos informa uma medida de tendência central do tamanho das perdas que excedem o valor do quantil VaR. Ou seja, o VaR nos informa apenas que uma proporção $\alpha$ das perdas serão menores que a medida, mas nada nos informa se esta perda extraordinária de fato ocorrer. Mesmo sendo criticado e demonstradamente uma medida não coerente de risco, o VaR continua a ser amplamente utilizado, mesmo que agora em conjunto com o ES. 

%Mais recentemente o Comitê de Supervisão Bancária de Basileia tem se proposto a adotar o \emph{Expected Shortfall} como medida de risco de mercado. \cite{BankingSupervision2013a}. O Comitê cita a grande importância da escolha da medida de risco e sua calibração, e portanto estas são relevantes para as decisões de política do Banco. Entre as dificuldades encontradas pelo VaR estão mais notadamente sua inabilidade em estimar o "risco de cauda" da distribuição de perdas, uma vez que VaR não leva em conta a distribuição das perdas acima do valor de corte.

%Desta forma, foi decidido que o ES seria a medida de risco favorita para a abordagem pelo banco chamada de modelos internos. Ou seja, os bancos supervisionados devem utilizar o ES para o cálculo do risco de mercado a que estão sujeitos em seus modelos internos. O comitê também se decidiu por um intervalo de confiança de 97,5\% para o ES, em contraposição a 99\% para o VaR. O comitê espera que esta abordagem para o cálculo da medida de risco de mercado trará benefícios se comparada a antiga abordagem pelo Var, entre elas um modelo com resultados mais estáveis e menor sensibilidade a observações extremas (\emph{outliers}).

\paragraph{Teoria do valor extremo} EVT da sigla em inglês, é um ramo da estatística que lida diretamente com eventos raros, extremos. Seu objetivo é modelar o comportamento assintótico de eventos que se distanciam muito da mediana de uma distribuição. Justamente por esta característica, a EVT está sendo utilizada para modelar riscos que possuem distribuição com caudas longas, um dos fatos estilizados bem conhecidos sobre retornos de ativos financeiros.

Ao utilizar a EVT, e mais especificamente o método conhecido como \emph{peaks over treshold} – POT, se está interessado em modelar apenas a parte da cauda da distribuição das perdas de um ativo financeiro maiores que um determinado valor de limiar \emph{u}. É da modelagem desta cauda, portanto, que se calculam as estimativas de risco $VaR_\alpha$.

\todo{incluir as diversas literaturas sobre a aplicação da evt ao VaR}

\cite{McNeil2000}
\cite{Longin2000} \emph{stress test} utilizando evt
\cite{Wong2003} um estudo sobre as implicações da precisão do modelo VaR no gerenciamento do risco de mercado em bancos
\cite{Berkowitz2002} precisão dos modelos em bancos comerciais
\cite{Bystroem2004}
\cite{Gencay2004}
\cite{Kuester2006} comparação entre diversos modelos VaR
\cite{Herrera2013}
\cite{Rocco2014} uma revisão sobre o uso da evt em finanças
\cite{Karmakar2014}
\cite{Chavez-Demoulin2016}
\cite{Karmakar2016} aplicação do modelo evt condicional a retornos intra-diários
\cite{OBrien2017} avaliação dos modelos de risco de bancos no pré, durante e pós crise financeira de 2008

\section{Modelando caudas e medidas de risco associadas com EVT}
\label{sec:caudas}

Considere uma amostra de uma variável aleatória - \emph{va} - cujas observações sejam independentes e igualmente distribuídas - \emph{iid}, $X_i$, com $i\in \mathbb{N}$, que representem as perdas financeiras de um determinado ativo, com uma função de distribuição - \emph{df} - desconhecida $F(x) = P(X_i \leq x)$.
Seja \emph{u} um valor de limiar a partir do qual perdas acima deste valor sejam consideradas extremas. Os valores de excesso serão, portanto, $X_i - \emph{u}$.

A EVT está interessada em investigar o comportamento da distribuição dos máximos desta \emph{va} dados por $M_n = \max (X_1, \ldots , X_n)$ para vários valores de $n$ e a medida que $n\rightarrow \infty$. A sequência $M_n$ é chamada de máximos em bloco e foi demonstrado através do conhecido teorema de Fisher-Tippett-Gnedenko, \cite{Fisher1928}, \cite{Gnedenko1941, Gnedenko1943}, que a única distribuição para a qual $M_n$ converge com $n\rightarrow \infty$ é a distribuição de valores extremos generalizada. Se esta distribuição de valores extremos generalizada - GEV - for denotada por $H_\xi$, com $\xi$ um parâmetro da distribuição, então se diz que $F \in MDA(H_\xi)$, $F$ pertence ao domínio de máxima atração de $H_\xi$.

%Para tanto, é necessário normalizarmos esta sequência de máximos de forma que sua distribuição seja convergente para uma distribuição $H(x)$ não-degenerada. Seja $F(x)$ a distribuição original de uma variável aleatória \emph{iid}, é possível normalizar seus máximos em bloco através da relação $M_n^*=(M_n-d_n)/c_n$ de forma que:
%
%\begin{equation}
%\label{eq:H}
%\lim_{n \rightarrow \infty} P\left(\frac{M_n-d_n}{c_n} \leq x \right) 
%= \lim_{n \rightarrow \infty} F^n(c_nx + d_n)
%= H(x)
%\end{equation}

%Em outras palavras, para determinadas sequências $c_n$ e $d_n$ a serem escolhidas, existe uma distribuição de $H(x)$ não-degenerada a qual representa a distribuição dos máximos em bloco de $F(x)$.
%
%A potenciação de $F$ em $n$ deriva diretamente da suposição que a variável aleatória é iid, enquanto que a transformação de $x \rightarrow c_n x+d_n$ é a normalização dos máximos em bloco.

%\begin{defi}[MDA] \label{defi:MDA}
%	Domínio de máxima atração, se a equação \eqref{eq:H} é válida para uma $H$ não-degenerada, então se diz que $F \in MDA(H)$, $F$ pertence ao domínio de máxima atração de $H$.
%\end{defi}
%
%\begin{teor}[Fisher-Tippett, Gnedenko] \label{teor:fisher-tippett}
%	Se $F \in MDA(H)$ para alguma $H$ não-degenerada, então $H$ deve ser uma distribuição do tipo de valores extremos generalizada – GEV.
%\end{teor}
%O teorema \ref{teor:fisher-tippett} foi estabelecido através de três artigos, \cite{Fisher1928}, \cite{Gnedenko1941, Gnedenko1943}.

\begin{defi}[GEV] \label{defi:GEV}
	Distribuição de valores extremos generalizada, é definida por sua função densidade de probabilidade - pdf - a qual é dada por:
	
	\begin{equation}
	\label{eq:GEV}
	H_\xi(x) = 
	\begin{cases}
	exp(-(1+\xi x)^{-\frac{1}{\xi}}), & \xi \neq 0,\\
	exp(-e^{-x}), & \xi = 0,\\
	\end{cases}
	\end{equation}
\end{defi}

O parâmetro $\xi$ é conhecido como parâmetro de forma da distribuição e dependendo deste valor tem-se diferentes tipos de distribuição (casos particulares da GEV). Quando $\xi=0$ a distribuição resultante é uma Gumbel, quando  $\xi>0$ uma Fréchet surge, e por fim quando $\xi<0$ tem-se uma Weibull.

Para as aplicações financeiras não necessitamos calcular a qual $MDA$ pertencem nossas distribuições contínuas, bastando saber que basicamente todas as distribuições de utilidade prática estão contidas em $MDA(H_\xi)$ para algum valor de $\xi$ \cite[p. ~139]{McNeil2015}.

\subsection{Excessos acima de um limiar}
\label{sec:excess}

O método POT para calcular a função de distribuição dos valores que excedem um determinado limiar de um conjunto de dados vem sendo empregado no mundo financeiro para ajustar as caudas das distribuições de retornos, ou perdas, dos ativos. Este método é preferido a teoria clássica de valores extremos (e.g. máximos em bloco), pois, desperdiça uma quantidade menor de dados da série original. Qualquer valor que exceda o limiar pré-determinado é considerado na distribuição dos excessos. Esta é definida como.

\begin{defi}[Distribuição dos excessos] \label{defi:excess}
	Seja \emph{X} uma variável aleatória com função de distribuição \emph{F}. A distribuição dos excessos sobre um limiar \emph{u} tem a seguinte função de distribuição:
	
	\begin{equation}
	\label{eq:excessdist}
	F_u(x)=P(X-u \leq x | X > u)=\frac{F(x+u)-F(u)}{1-F(u)}
	\end{equation}
	para $0 \leq x < x_F-u$, onde $x_F \leq \infty$ é o limite direito da distribuição \emph{F}.
\end{defi}

Uma importante distribuição que surge na modelagem dos excessos sobre um limiar é a distribuição gereralizada de Pareto – GPD, que segue.

\begin{defi}[GPD] \label{defi:GPD}
	Distribuição de Pareto Generalizada é definida por sua função de distribuição:
	\begin{equation}
	\label{eq:GPD}
	G_{\xi,\psi}(x) = 
	\begin{cases}
	1- \left(1+ \frac{\xi x}{\psi} \right)^{-\frac{1}{\xi}}, & \xi \neq 0,\\
	1-exp\left(-\frac{x}{\psi}\right), & \xi = 0,\\
	\end{cases}
	\end{equation}
	onde $\psi > 0$, e $x\geq 0$ quando $\xi  \geq 0$ ou $0 \leq x \leq -\psi / \xi$ quando $\xi < 0$.
\end{defi}

Os parâmetros $\xi$ e $\psi$ são conhecidos respectivamente como parâmetros de forma e escala da distribuição. A GPD tem papel fundamental na teoria de valor extremo em função do teorema de Pickands-Balkema-de Haan, \cite{Pickands1975} e \cite{Balkema1974}, pois estes demonstram que, para um valor suficientemente alto do limiar \emph{u}, a distribuição dos excessos $F_u(x)$ pode ser aproximada por uma GPD $G_{\xi,\psi}(x)$.

%\begin{teor}[Pickands-Balkema-de Haan]
%	\label{teor:pickands}
%	Pode ser encontrada uma função $\psi(u)$ tal que:
%	\begin{equation*}
%	\lim\limits_{u \rightarrow x_F} \; \sup\limits_{0\leq x <x_F - u} |F_u(x)-G_{\xi, \psi(u)}(x)| = 0 
%	\end{equation*}
%	se e somente se $F\in MDA(H_\xi)$ para $\xi \in \mathbb{R}$.
%\end{teor}

O que este teorema prova é que para distribuições as quais os máximos em bloco normalizados convergem para uma GEV (na forma da equação \eqref{eq:GEV}), então a distribuição dos excessos acima de um limiar destas mesmas distribuições convergem para uma GPD, dado um valor de limiar \emph{u} adequado. Como para fins práticos basicamente todas as distribuições contínuas de fato estão no $MDA(H_\xi)$ para algum valor de $\xi$, temos que a GPD é a distribuição a ser escolhida para modelar excessos acima de um limiar.

Ao se fazer esta suposição que a distribuição dos excessos \emph{é igual} a uma GPD, pode-se então a partir dos dados de perdas estimar os parâmetros de forma e escala e, portanto, modelar a cauda direita da distribuição de perdas de forma parametrizada com o auxílio da equação \eqref{eq:excessdist}. 


Dada a parametrização de uma GPD, é interessante saber o valor esperado desta distribuição, uma vez que esta medida de valor central fornece importante informação sobre a quantidade de risco que se está medindo, assim como a informação de que a própria distribuição foi ajustada aos dados de forma satisfatória.

O valor esperado de uma variável aleatória não negativa pode ser computado através da integral de sua cauda, $P(X>x) = 1-P(X \leq x)$. A cauda da GPD é, para $\xi \neq 0$, $\left(1+\xi x / \psi \right)^{-1/ \xi}$.

%Bastando, portanto, integrar em relação a $x$ sobre o domínio deste, que é de $0$ a $\infty$.
%
%\begin{equation*}
%\displaystyle\int\limits_{0}^{\infty} \left(1+ \xi x /\beta(u) \right)^{-1/\xi} dx
%\end{equation*}

Desta forma, o valor esperado de uma GPD, $G_{\xi,\psi}(x)$, converge para valores de $\xi<1$ e é dado pela seguinte equação:

\begin{equation}
\label{eq:meanGPD}
E\left[G_{\xi,\psi} (X) \right]=\frac{\psi}{1-\xi}
\end{equation}


\begin{defi}[função média dos excessos]
	\label{defi:meanexcess}
	A função média dos excessos de uma variável aleatória \emph{X} com média finita é dada por:

	\begin{equation}
	\label{eq:meanexcess}
	e(u)=E\left(X-u | X > u\right)
	\end{equation}
\end{defi}

Ou seja, a equação \eqref{eq:meanexcess} representa o valor esperado da função de distribuição dos excessos dada pela Definição \ref{defi:excess}. Ela representa a média de $F_u$ como uma função do limiar \emph{u}. Por vezes também é conhecida como função média de vida residual (\emph{mean residual life function}).

Para uma variável distribuída na forma de uma GPD, o parâmetro de escala é uma função linear em \emph{u} dado por $\psi(u)=\psi + \xi u$, \cite[Teorema 3.4.13(e)]{Embrechts1997}. Utilizando-se deste fato e da equação \eqref{eq:meanGPD} chegamos ao cálculo da função média dos excessos para uma GPD, dada por:

\begin{equation}
\label{eq:eu}
e(u)=\frac{\psi+\xi u}{1-\xi}
\end{equation}
onde $0 \leq u < \infty$ se $0 \leq \xi <1$ e $0 \leq u \leq -\psi / \xi$ se $\xi < 0$. É possível observar que de fato a função média dos excessos em uma GPD é linear em \emph{u}. Esta é uma característica importante de uma GPD e que pode ser utilizada para auxiliar a escolha de um valor adequado do limiar \emph{u} de tal forma que a suposição de convergência $F_u(x) \rightarrow G_{\xi, \psi}(x)$ seja válida.

Assim, quando da análise de uma determinada distribuição de perdas \emph{F} e deseja-se ajustar a cauda desta distribuição, ou seja, as perdas acima de um dado valor limiar \emph{u} a uma GPD $G_{\xi, \psi}(x)$ é necessário primeiramente determinar um valor adequado de \emph{u} garantirmos a suposição de convergência. Um método frequentemente utilizado é o gráfico da função média dos excessos com relação a \emph{u}. Analisando este gráfico, escolhe-se o menor valor de \emph{u} para o qual a partir deste ponto a relação $e(u) \text{ vs } u$ torna-se linear.

Deseja-se o menor valor de \emph{u} para o qual a relação é linear pois, mesmo o método POT implica em grande perda de dados da série temporal, já que apenas os valores acima deste limiar são utilizados para fazer a estimação dos parâmetros $\xi$ e $\psi$ da GPD. Portanto, existe um \emph{trade-off} na escolha do valor limiar \emph{u}, escolhendo um valor muito baixo obtém-se uma boa quantidade de dados para estimar os parâmetros da GPD, mas a própria distribuição resultante não será GPD, uma vez que não estaremos trabalhando na região onde a relação $e(u) \text{ vs } u$ é linear, e portanto os parâmetros estimados serão viesados. Por outro lado, um valor limiar muito alto impõe o custo de trabalhar com poucos dados para fazer a estimação dos parâmetros da distribuição e por conseguinte, os erros padrões dessas estimativas serão elevados.

\todo{incluir parágrafo sobre quantidade ideal de observações para estimar gpd. McNeil20015 tem algo sobre.}

%\begin{lema} \label{lema:ev}
%	Sob a Suposição \ref{supo:excessdist} segue que $F_v (x)=G_{\xi,\psi+\xi(v-u)} (x)$ para qualquer valor limiar $v \geq u$.
%\end{lema}
%
%Logo, a distribuição dos excessos sobre limiares mais altos que \emph{u}, também segue uma GPD com o mesmo parâmetro de forma $\xi$ e parâmetro de escala que cresce linearmente com este limiar mais alto \emph{v}. Se $\xi < 1$, a média desta nova GPD converge e é dada por:
%
%\begin{equation}
%\label{eq:ev}
%e(v)=\frac{\psi+\xi(v-u)}{1-\xi}=\frac{\xi v}{1- \xi}+ \frac{\psi-\xi u}{1-\xi}
%\end{equation}
%
%Esta é a função média dos excessos sobre limiares mais altos, e está definida para $u \leq v < \infty$ se $0 \leq \xi < 1$ e, $u \leq v \leq u-\psi / \xi$ se $\xi < 0$.
%
%Esta função é muito útil para calcularmos o $ES_\alpha$ (\emph{expected shortfall}), considerando que $VaR_\alpha$ nada mais é que um quantil superior ao limiar $u$ escolhido.

\subsection{Estimando o VaR}

Através da modelagem da cauda da distribuição \emph{F} de perdas por uma GPD, é possível calcular a medida de risco $VaR_\alpha$ em função dos parâmetros da GPD estimada e também fazendo uso da distribuição empírica de \emph{F}.

Sob a suposição de convergência a cauda da distribuição \emph{F}, $\bar{F}(x)$, para $x \geq u$ é dada por:

\begin{align}
\label{eq:Ftail}
\bar{F}(x) & = P(X>u)P(X>x|X>u) \nonumber \\
& = \bar{F}(u) P(X-u>x-u|X>u) \nonumber \\
& = \bar{F}(u)\bar{F}_u(x-u) \nonumber \\
& = \bar{F}(u)\left(1+\xi \frac{x-u}{\psi}\right)^{-1/\xi}
\end{align}

Aqui $x$ são os valores a serem observados das perdas, e portanto $x-u$ são as perdas em excesso ao limiar.

A equação \eqref{eq:Ftail} efetivamente separou a distribuição \emph{F}, ou melhor, sua cauda, em duas partes. A primeira parte, para valores menores que \emph{u}, não foi modelado analiticamente e portanto utiliza-se a distribuição empírica das perdas, aqui representada por sua cauda $\bar{F}(u)$, que nada mais é que o número observado de excessos de \emph{u} sobre o número total de observações da amostra.

A segunda parte é justamente a modelagem através de uma GPD com parâmetros $\xi$ e $\psi$ dado o limiar \emph{u}. Por esta modelagem paramétrica pode-se conhecer as probabilidades de cauda para valores de \emph{x} maiores que \emph{u}.

O quantil $\alpha$ é a inversa da função distribuição e retorna o valor para o qual um percentual $\alpha$ de observações da amostra é menor ou igual. Como se está trabalhando com a distribuição de perdas, $VaR_\alpha$ nada mais é que um quantil alto para o qual $\alpha \%$ das perdas devem ser menores ou iguais a este valor.

Como a equação \eqref{eq:Ftail} fornece a probabilidade de cauda, então esta é igual a $1- \alpha$ para um valor de $\alpha  \geq F(u)$. O valor $1- \alpha$ é conhecido como a cobertura da medida de risco. Fazendo $\bar{F}(x)=1-\alpha$ na equação \eqref{eq:Ftail} o valor de \emph{x} representará $VaR_\alpha$ e basta manipular esta equação para encontrar $VaR_\alpha$ como função de $\bar{F}(u)$, $\alpha$ e dos parâmetros da GPD $\xi$ e $\psi$. Que garante a equação abaixo:

\begin{equation}
\label{eq:VaRGPD}
VaR_\alpha = q_\alpha(F) = u+\frac{\psi}{\xi}\left[ \left( \frac{1-\alpha}{\bar{F}(u)}\right)^{-\xi}-1 \right]
\end{equation}

A medida $ES_\alpha$ pode ser entendida como a média das perdas que excedem o valor dado por $VaR_\alpha$. Como o próprio $VaR_\alpha$ é um quantil acima do valor de limiar \emph{u}, $ES_\alpha$ é dado pelo valor do $VaR_\alpha$ somado a função média dos excessos dada pela equação \eqref{eq:ev} fazendo $v = VaR_\alpha$. Esta média é convergente para valores de $\xi < 1$ conforme já demonstrado. Ou seja, $ES_\alpha=VaR_\alpha + e(VaR_\alpha)$. A qual nos rende de forma mais geral:

\begin{equation}
\label{eq:ESGPD}
ES_\alpha = \frac{VaR_\alpha}{1-\xi}+\frac{\psi-\xi u}{1-\xi}
\end{equation}

Portanto, ambas medidas de risco $VaR_\alpha$ e $ES_\alpha$, para distribuições de perdas que tiveram suas caudas modeladas através de uma GPD da forma $G_{\xi, \psi(u)}$ com $\xi <1 \text{ e } \psi > 0$, podem ser calculadas respectivamente através das equações dadas em \eqref{eq:VaRGPD} e \eqref{eq:ESGPD}. As estimativas destas medidas de risco serão encontradas através das estimativas dos parâmetros da GPD, assim como do limiar utilizado e de uma medida empírica de $\bar{F}(u)$ que será o número de excessos verificados sobre o total de amostras. É claro que, ao adotarmos esta estimativa para $\bar{F}(u)$ estamos implicitamente supondo que o número de amostras na série de perdas é significativa, assim como o número de excessos verificados. Daí a importância de se utilizar um valor \emph{u} adequado, conforme explicitado na seção \ref{sec:excess}.

As estimativas de medidas de risco desenvolvidas nesta seção se qualificam como medidas incondicionais, no sentido que elas não dependem do estado atual das coisas, mas sim de todo o histórico de eventos de forma uniforme. Em outras palavras, $VaR_\alpha \text{ e } ES_\alpha$ derivados a partir das equações \eqref{eq:VaRGPD} e \eqref{eq:ESGPD} são medidas históricas de risco associado ao ativo em análise, entretanto, não levam em consideração se nos eventos mais recentes a volatilidade das perdas pode ser diferente do valor histórico.

De fato, uma das características marcantes das perdas (ou equivalentemente, dos retornos) dos ativos financeiros é o chamado \emph{clustering} de volatilidade, onde grandes volatilidades (retornos positivos ou negativos) têm tendência a ficarem próximas ao longo da linha temporal. Em geral estas aglomerações de volatilidades surgem a partir da autocorrelação destas, ou seja, a volatilidade em um período \emph{t} é dependente das volatilidades verificadas em períodos anteriores. Um modelo bastante encontrado na literatura que busca modelar estas dependências é o modelo ARCH e suas variantes como GARCH, propostos por \cite{Engle1982} e \cite{Bollerslev1986} respectivamente.

Assim, ao passo que as estimativas de risco desenvolvidas nesta seção são valiosas para prazos mais longos, ainda é necessário desenvolver um modelo que lide com o fato das autocorrelações de volatilidades e portanto, que nossa variável aleatória não é independente e igualmente distribuída ao longo do tempo. O modelo proposto por \cite{McNeil2000} pode ser utilizado para encontrar as medidas de risco $VaR_\alpha$ e $ES_\alpha$ condicionais que desejamos, ainda dentro da metodologia de \emph{peaks over treshold} - POT.

\subsection{Modelo eGARCH-EVT}
\label{sec:egarchevt}

Ativos financeiros possuem características de autocorrelação, senão em seus retornos propriamente ditos, ao menos em suas volatilidades ou variações absolutas. Ou seja, dada uma grande variação no momento \emph{t} é de se esperar novamente uma grande variação, não necessariamente na mesma direção daquela anterior, para o momento \emph{t+1} e posteriores. Desta forma, medidas de risco incondicionais, conforme aquelas derivadas na Seção \ref{sec:caudas} podem ser adequadas somente para horizontes temporais mais longos, pois implicitamente tomam em consideração os fatos mais recentes com o mesmo valor de predição que fatos mais longínquos.

Assim, nesta subseção trabalharemos com um modelo semelhante ao proposto por \cite{McNeil2000} os quais fazem uma adequação dos retornos dos ativos a um modelo AR-GARCH e posteriormente tratam os erros desta modelagem como \emph{iid} e portanto, a metodologia de POT pode ser aplicada.

Primeiramente vamos estabelecer um modelo eGARCH para as perdas do ativo subjacente. Se denotarmos $L_t$ como sendo a perda observada no período \emph{t}, $\mu_t$ e $\sigma_t$ são respectivamente a média e o desvio padrão condicionais e mensuráveis através do conjunto de informações disponíveis em \emph{t-1} e seja $Z_t$ inovações \emph{iid} com média zero e desvio padrão unitário, então temos que:

\begin{equation}
\label{eq:loss}
L_t=\mu_t+\sigma_t Z_t
\end{equation}

Seja $F_L(l)$ a distribuição marginal de $L_t$, então $F_{L_{t+1} | \mathcal{G}_t}(l)$ é a distribuição preditiva da perda para o próximo período, onde $\mathcal{G}_t$ é o conjunto de informações disponíveis no período \emph{t}, incluindo-o. Portanto, para o cálculo das medidas condicionais de risco estamos interessados em um quantil $\alpha$ na cauda de $F_{L_{t+1} | \mathcal{G}_t}(l)$. Este quantil $\alpha$, que será o nosso $VaR_\alpha$, é o ínfimo \emph{l} tal que o valor da distribuição preditiva seja maior ou igual a $\alpha$. Ao passo que o valor condicional do \emph{ES} será o valor esperado das perdas previstas que sejam maiores que VaR para o mesmo intervalo de confiança. Ou seja:

\begin{align}
VaR_\alpha^t=&\inf\{l \in \mathbb{R}: F_{L_{t+1}} | \mathcal{G}_t(l) \geq \alpha\}, \\
ES_\alpha^t=&E[L_{t+1} | L_{t+1} > VaR_\alpha^t]
\end{align}

Considerando que nossa distribuição de perdas é dada pela equação \eqref{eq:loss} e sabendo das propriedades de variáveis aleatórias e do operador de expectância, as equações dadas acima subsumem a:

\begin{align}
VaR_\alpha^t=&\mu_{t+1}+\sigma_{t+1}z_\alpha, \label{eq:varcond} \\
ES_\alpha^t=&\mu_{t+1}+\sigma_{t+1}E[Z | Z>z_\alpha] \label{eq:escond}
\end{align}
onde $z_\alpha$ é o quantil $\alpha$ das inovações \emph{Z}.

Agora nos falta escolher um processo que modele nossa série temporal dada em \eqref{eq:loss}, ou seja, precisamos especificar o comportamento de $\mu_t$ e $\sigma_t$. Por suposição do modelo, especificamos que o comportamento destas variáveis é dependente de acontecimentos passados, contidos no conjunto de informações  $\mathcal{G}_{t-1}$ . Dentre os diversos modelos já propostos para estimar médias e volatilidades condicionais, está o simples porém efetivo modelo eGARCH(1,1) para a volatilidade condicional e o modelo ARMA(1,2) para a média condicional. Uma extensão destes modelos básicos para outros mais complexos pode ser facilmente obtida e é vasta na literatura, como exemplo modelos GARCH-M, \emph{Treshold} GARCH, EGARCH, etc. e modelos do tipo ARIMA, entre outros encontrados em \cite{Tsay2010}. \todo{Implementar com modelo EGARCH}

Neste trabalho, visando aplicar a teoria do valor extremo para o cálculo das medidas condicionais de risco, não faremos maiores assunções acerca da distribuição das inovações, como por exemplo assumir uma determinada distribuição (e.g. Normal ou t de Student), mas está implícito que esta pertence ao \emph{MDA} de uma GEV e portanto a distribuição de seus excessos sobre um limiar segue aproximadamente uma GPD.

Dadas estas considerações, o modelo adotado segue um formato em dois estágios para ser implementado, como segue.
\begin{itemize}
	\item Ajustar um modelo eGARCH para os dados de perdas, sem fazer suposições sobre a distribuição de \emph{Z}, porém, utilizando um método de pseudo máxima verossimilhança (PML). Deste modelo tiramos as estimativas de $\mu_{t+1}$ e $\sigma_{t+1}$ e portanto, calculamos as inovações implícitas resultantes através da equação \eqref{eq:loss}.
	\item Consideramos estas inovações calculadas como sendo as realizações da variável aleatória \emph{Z}, a qual por suposição pode ter sua cauda ajustada a uma GPD utilizando o método descrito na seção \ref{sec:caudas}. Encontraremos por fim o valor de $z_\alpha$ e $E[Z|Z>z_\alpha]$, com os quais finalizamos os cálculos de nossas medidas condicionais de risco dadas em \eqref{eq:varcond} e \eqref{eq:escond}.
\end{itemize}

Nosso modelo completo para as medidas de risco $VaR_\alpha$ e $ES_\alpha$ condicionais dada a distribuição de perdas $L_t$ de um ativo será, portanto:

\begin{align}
L_t=&\mu_t+\epsilon_t \\
\mu_t=&\mu+ \phi_1 L_{t-1}+ \theta_1\epsilon_{t-1}+ \theta_2\epsilon_{t-2} \label{eq:mut} \\
\epsilon_t=&\sigma_t Z_t\\
\ln(\sigma_t^2)=&\omega+ \alpha_1 Z_{t-1}+ \gamma_1(|Z_{t-1}|-E|Z_{t-1}|)+ \beta_1 \ln(\sigma_{t-1}^2) \label{eq:sigma2} \\
Z_t\sim &\mathcal{D}(0,1) \text{ e } \mathcal{D} \in MDA(H_\xi)
\end{align}
com as equações \eqref{eq:varcond} e \eqref{eq:escond} nos fornecendo os valores das medidas $VaR_\alpha$ e $ES_\alpha$ respectivamente, quando utilizadas em conjunto com aquelas dadas por \eqref{eq:VaRGPD} e \eqref{eq:ESGPD}.

O termo de erro $\epsilon_t$ será uma série estritamente estacionária se $\beta_1+\alpha_1<1$, o que garante que a variância de nosso modelo ($\sigma_t^2$) será finita, apesar de variável no tempo.

Este modelo é ajustado utilizando-se o método de pseudo máxima verossimilhança, o que significa que, apesar de não adotarmos a distribuição normal para reger as inovações $Z_t$, a utilizamos apenas para estimar os parâmetros do modelo. É demonstrado na literatura, \cite[Capítulo 4]{Gourieroux1997}, que o método PML fornece estimadores consistentes e assintoticamente normais, assim podemos utilizar tal método para obter nossas estimativas de parâmetros e erros padrão, mesmo não aceitando a condição de normalidade das inovações.

\section{Dados utilizados e estatísticas descritivas}

\section{Resultados empíricos}

\input{tables/artigo-tab-descritivas.tex}

\input{tables/artigo-tab-garchcoef}

\input{tables/artigo-tab-garchstats}

\input{tables/artigo-tab-evtcoef}	

\input{tables/artigo-tab-varviol}

\input{tables/artigo-tab-vartest}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/artigo-retornos}
	\caption{Retornos dos índices do estudo. Período completo entre 01/01/2003 a 30/08/2017.}
	\label{fig:artigo-retornos}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/artigo-qqplots}
	\caption{Análise de normalidade dos retornos através de gráficos quantil-quantil.}
	\label{fig:artigo-qqplots}
\end{figure}

% Figuras com os gráficos ACF antes e depois da filtragem eGARCH
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/artigo-acf-IBovespa}
	\caption{ACF dos retornos observados e seus quadrados, e ACF dos resíduos padronizados e seus quadrados após a modelagem eGARCH. Período dentro da amostra, Ibovespa 01/01/2013 a 31/12/2008.}
	\label{fig:artigo-acf-ibovespa}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/artigo-acf-IPC}
	\caption{ACF dos retornos observados e seus quadrados, e ACF dos resíduos padronizados e seus quadrados após a modelagem eGARCH. Período dentro da amostra, IPC 01/01/2013 a 31/12/2008.}
	\label{fig:artigo-acf-ipc}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/artigo-acf-IPSA}
	\caption{ACF dos retornos observados e seus quadrados, e ACF dos resíduos padronizados e seus quadrados após a modelagem eGARCH. Período dentro da amostra, IPSA 01/01/2013 a 31/12/2008.}
	\label{fig:artigo-acf-ipsa}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/artigo-acf-Merval}
	\caption{ACF dos retornos observados e seus quadrados, e ACF dos resíduos padronizados e seus quadrados após a modelagem eGARCH. Período dentro da amostra, Merval 01/01/2013 a 31/12/2008.}
	\label{fig:artigo-acf-merval}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/artigo-acf-SP500}
	\caption{ACF dos retornos observados e seus quadrados, e ACF dos resíduos padronizados e seus quadrados após a modelagem eGARCH. Período dentro da amostra, S\&P500 01/01/2013 a 31/12/2008.}
	\label{fig:artigo-acf-sp500}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/artigo-acf-SP-TSE}
	\caption{ACF dos retornos observados e seus quadrados, e ACF dos resíduos padronizados e seus quadrados após a modelagem eGARCH. Período dentro da amostra, S\&P TSE 01/01/2013 a 31/12/2008.}
	\label{fig:artigo-acf-sptse}
\end{figure}



\section{Conclusão}

\section*{Referências}

\bibliography{library}

\end{document}
